# makemore

**makemore is a repository that aims to build character-level language models from scratch**, following a step-by-step approach with clear explanations. This project starts with simple models and progresses towards more complex neural network architectures, including a Transformer equivalent to GPT-2.

## What is makemore?

As the name suggests, **makemore learns from a given dataset and then generates more of similar things**. The initial focus is on **character-level language modeling**, where the model treats each input text (like a name in `names.txt`) as a sequence of individual characters and learns to predict the next character in the sequence.

## Dataset

The primary dataset used in the initial stages is **`names.txt`**, which contains a large collection of names (approximately 32,000) found randomly on a government website. By training makemore on this dataset, it learns to generate new, unique, name-like strings.

## Implemented and Planned Models

This project will implement a variety of character-level language models, including:

*   **Bi-gram model:** A simple model that predicts the next character based only on the previous character.
*   **Bag-of-words models**.
*   **Multilayer Perceptrons (MLPs)**.
*   **Recurrent Neural Networks (RNNs)**.
*   **Transformers:** Culminating in a Transformer architecture similar to GPT-2.

Future extensions may include:

*   **Word-level language models:** To generate larger documents composed of words.
*   **Image and image-text networks:** Exploring models like DALL-E and Stable Diffusion.

## Getting Started

The initial steps involve:

1.  **Loading the dataset:** Reading the `names.txt` file and splitting it into a list of individual words.
2.  **Exploring the data:** Examining the number of words, shortest word length, and longest word length to understand the dataset.

## First Steps: Building a Bi-gram Language Model

The first language model built in this project is a **bi-gram model**. This model operates by looking at pairs of consecutive characters (bi-grams) within the training data to understand the likelihood of one character following another.

### Counting Bi-grams

The process involves:

1.  Iterating through each word in the dataset.
2.  Creating bi-grams from consecutive characters in each word.
3.  Including special start and end tokens (initially '[' and ']') to capture the beginning and end of words.
4.  Counting the occurrences of each bi-gram to learn their statistical relationships.
5.  Storing these counts in a dictionary or a 2D array (tensor) for efficient access.

### Probability Calculation

The bi-gram counts are then used to calculate the probability of the second character given the first character. This is done by normalizing the counts for each preceding character (row in the 2D array) by the total count of characters that follow it.

### Sampling New Names

Once the probabilities are established, new name-like strings can be generated by:

1.  Starting with the special start token.
2.  Sampling the next character based on the probability distribution for the start token.
3.  Using the newly sampled character to sample the subsequent character, and so on.
4.  Stopping the generation when the special end token is sampled.

### Evaluating the Model: Negative Log Likelihood

The quality of the bi-gram model can be evaluated using the **negative log likelihood** on the training data.

1.  For each bi-gram in the training data, the probability assigned by the model to that bi-gram is retrieved.
2.  The logarithm of this probability is calculated.
3.  The average of the negative of these log probabilities over the entire training set provides a measure of the model's performance (loss). **Lower loss indicates a better model**.

### Model Smoothing

To avoid assigning zero probability to unseen bi-grams, a technique called **model smoothing** can be applied. This involves adding a small, constant count (e.g., 1) to all bi-gram counts before calculating probabilities. This ensures that all bi-grams have a non-zero probability.

## Transitioning to Neural Networks

The project then transitions to implementing bi-gram character-level language models using a **neural network framework (PyTorch)**.

### Neural Network Architecture

The initial neural network for the bi-gram model consists of a single linear layer followed by a softmax function.

1.  **Input Encoding:** Each input character is converted into a **one-hot encoded vector**. This vector has a length equal to the number of unique characters (27, including the special token), with a '1' at the index corresponding to the character and '0' elsewhere.
2.  **Linear Layer:** The one-hot encoded input vector is fed into a linear layer with weights (**W**). The output of this layer represents **logits** (log-counts) for the next character. Due to the nature of one-hot encoding, the output logits effectively correspond to a specific row in the weight matrix **W**.
3.  **Softmax:** The logits are passed through a **softmax function** to produce a **probability distribution** over the next possible characters. The softmax function exponentiates the logits and then normalizes them so that they sum to 1.

### Training with Gradient Descent

The neural network's weights (**W**) are learned using **gradient-based optimization** to minimize the **negative log likelihood loss** on the training data.

1.  **Forward Pass:** The input bi-grams are processed through the neural network to obtain probability distributions for the next character. The negative log likelihood loss is calculated by comparing these predictions with the actual next characters in the training data.
2.  **Backward Pass:** The gradients of the loss with respect to the network's weights are calculated using **backpropagation**.
3.  **Parameter Update:** The weights are updated in the opposite direction of their gradients to reduce the loss. This process is repeated iteratively until the model converges to a satisfactory loss value.